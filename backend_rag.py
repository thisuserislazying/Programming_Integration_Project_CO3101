import sys
import os
import json
import re

# Fix loi Windows (resource module)
if os.name == 'nt':
    import types
    module = types.ModuleType("resource")
    module.RLIMIT_NOFILE = 1
    module.setrlimit = lambda *args, **kwargs: None
    module.getrlimit = lambda *args, **kwargs: (0, 0)
    sys.modules["resource"] = module

from dotenv import load_dotenv
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles 
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from google.genai.errors import ServerError

from llama_index.core import StorageContext, load_index_from_storage, Settings
from llama_index.embeddings.google_genai import GoogleGenAIEmbedding
from llama_index.llms.google_genai import GoogleGenAI
from llama_index.core.memory import ChatMemoryBuffer
from llama_index.core.postprocessor import SentenceTransformerRerank
from llama_index.retrievers.bm25 import BM25Retriever
from llama_index.core.retrievers import QueryFusionRetriever
from llama_index.core.chat_engine import ContextChatEngine

# Cau hinh
load_dotenv()
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

app = FastAPI()
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

# Khoi tao AI models
embed_model = GoogleGenAIEmbedding(model_name="models/text-embedding-004", api_key=GEMINI_API_KEY)
llm = GoogleGenAI(model="gemini-2.5-flash", api_key=GEMINI_API_KEY, temperature=0.1)
Settings.embed_model = embed_model
Settings.llm = llm
reranker = SentenceTransformerRerank(model="cross-encoder/ms-marco-MiniLM-L-6-v2", top_n=3)

# Prompt he thong
SYSTEM_PROMPT = (
    "B·∫°n l√† m·ªôt gi·∫£ng vi√™n m√¥n Kinh t·∫ø Ch√≠nh tr·ªã M√°c - L√™nin uy t√≠n v√† nhi·ªát huy·∫øt t·∫°i ƒê·∫°i h·ªçc B√°ch Khoa.\n"
    "Nhi·ªám v·ª• c·ªßa b·∫°n l√† gi·∫£i th√≠ch c√°c kh√°i ni·ªám v√† tr·∫£ l·ªùi sinh vi√™n d·ª±a tr√™n ng·ªØ c·∫£nh (Context) ƒë∆∞·ª£c cung c·∫•p.\n"
    "Quy t·∫Øc quan tr·ªçng:\n"
    "1. Lu√¥n ∆∞u ti√™n d√πng th√¥ng tin t·ª´ gi√°o tr√¨nh ƒë∆∞·ª£c cung c·∫•p ƒë·ªÉ tr·∫£ l·ªùi.\n"
    "2. N·∫øu ng·ªØ c·∫£nh kh√¥ng c√≥ th√¥ng tin, h√£y n√≥i r√µ l√† gi√°o tr√¨nh kh√¥ng ƒë·ªÅ c·∫≠p, ƒë·ª´ng t·ª± b·ªãa.\n"
    "3. Gi·∫£i th√≠ch kƒ© l∆∞·ª°ng, s∆∞ ph·∫°m, c√≥ v√≠ d·ª• minh h·ªça th·ª±c t·∫ø kh√°c v·ªõi gi√°o tr√¨nh.\n"
    "4. Gi·ªçng vƒÉn: Th√¢n thi·ªán, d·ªÖ hi·ªÉu, khuy·∫øn kh√≠ch tinh th·∫ßn h·ªçc t·∫≠p. X∆∞ng h√¥: 'm√¨nh' - 'b·∫°n'.\n"
    "5. [QUAN TR·ªåNG] V·ªÅ ƒë·ªãnh d·∫°ng:\n"
    "   - TUY·ªÜT ƒê·ªêI KH√îNG s·ª≠ d·ª•ng d·∫•u thƒÉng (#, ##, ###) ƒë·ªÉ l√†m ti√™u ƒë·ªÅ.\n"
    "   - H√£y s·ª≠ d·ª•ng ch·ªØ IN ƒê·∫¨M ƒë·ªÉ l√†m ti√™u ƒë·ªÅ c√°c m·ª•c l·ªõn.\n"
    "   - S·ª≠ d·ª•ng d·∫•u g·∫°ch ngang (-) kh√¥ng s·ª≠ d·ª•ng c√°c d·∫•u (.) hay d·∫•u (*).\n"
    "6. Khi ƒë∆∞a ra v√≠ d·ª• minh h·ªça th√¨ h√£y l·∫•y c√°c v√≠ d·ª• th·ª±c t·∫ø, d·ªÖ h√¨nh dung ngo√†i gi√°o tr√¨nh.\n"
    "7. H√£y chia c√°c √Ω ch√≠nh th√†nh c√°c ƒëo·∫°n nh·ªè, xu·ªëng d√≤ng h·ª£p l√Ω ƒë·ªÉ d·ªÖ ƒë·ªçc.\n"
)

# Bien toan cuc
GLOBAL_ENGINE = None  # Engine dung chung cho tat ca moi nguoi
response_cache = {}
CACHE_FILE = "cache_answers.json"
index = None # Them bien index de quan ly

# Khoi tao he thong (Cache & Data & Engine)
def init_system():
    global GLOBAL_ENGINE, response_cache, index
    
    # Load cache
    if os.path.exists(CACHE_FILE):
        try:
            with open(CACHE_FILE, "r", encoding="utf-8") as f:
                response_cache = json.load(f)
        except: response_cache = {}

    # Load du lieu & Khoi tao Engine
    if os.path.exists("./storage"):
        try:
            ctx = StorageContext.from_defaults(persist_dir="./storage")
            index = load_index_from_storage(ctx)
            
            # Tao Retriever Hybrid
            vector_retriever = index.as_retriever(similarity_top_k=10)
            final_retriever = vector_retriever
            
            try:
                nodes = list(index.docstore.docs.values())
                bm25_retriever = BM25Retriever.from_defaults(
                    nodes=nodes, similarity_top_k=10, language="vi"
                )
                final_retriever = QueryFusionRetriever(
                    [vector_retriever, bm25_retriever],
                    similarity_top_k=15, num_queries=1, use_async=True
                )
            except: pass

            # Khoi tao Engine toan cuc 1 lan duy nhat
            GLOBAL_ENGINE = ContextChatEngine.from_defaults(
                retriever=final_retriever,
                llm=llm,
                memory=ChatMemoryBuffer.from_defaults(token_limit=3000),
                system_prompt=SYSTEM_PROMPT,
                node_postprocessors=[reranker]
            )
            
        except Exception as e:
            print(f"L·ªói kh·ªüi t·∫°o: {e}")

init_system()

# Luu cache xuong file
def save_cache():
    try:
        with open(CACHE_FILE, "w", encoding="utf-8") as f:
            json.dump(response_cache, f, ensure_ascii=False, indent=4)
    except: pass

# Chuan hoa cau hoi
def normalize_query_key(query: str) -> str:
    q = query.lower()
    q = re.sub(r'[^\w\s]', ' ', q)
    q = re.sub(r'\s+', ' ', q)
    return q.strip()

# Lam dep ten file
def prettify_source(file_name):
    name = file_name.replace(".txt", "")
    mapping = {
        "Chuong_0": "Ch∆∞∆°ng 0: PH·∫¶N M·ªû ƒê·∫¶U / KH√ÅI QU√ÅT",
        "Chuong_1": "Ch∆∞∆°ng 1: ƒê·ªêI T∆Ø·ª¢NG, PH∆Ø∆†NG PH√ÅP & CH·ª®C NƒÇNG C·ª¶A KTCT M√ÅC - L√äNIN",
        "Chuong_2": "Ch∆∞∆°ng 2: KINH T·∫æ TH·ªä TR∆Ø·ªúNG & C√ÅC QUY LU·∫¨T C∆† B·∫¢N",
        "Chuong_3": "Ch∆∞∆°ng 3: L√ù LU·∫¨N C·ª¶A C.M√ÅC V·ªÄ GI√Å TR·ªä TH·∫∂NG D∆Ø",
        "Chuong_4": "Ch∆∞∆°ng 4: T√çCH L≈®Y & T√ÅI S·∫¢N XU·∫§T TRONG N·ªÄN KTTT",
        "Chuong_5": "Ch∆∞∆°ng 5: C·∫†NH TRANH, ƒê·ªòC QUY·ªÄN & VAI TR√í C·ª¶A NH√Ä N∆Ø·ªöC",
        "Chuong_6": "Ch∆∞∆°ng 6: KINH T·∫æ TH·ªä TR∆Ø·ªúNG ƒê·ªäNH H∆Ø·ªöNG XHCN ·ªû VI·ªÜT NAM",
        "Chuong_7": "Ch∆∞∆°ng 7: L·ª¢I √çCH KINH T·∫æ & H√ÄI H√íA QUAN H·ªÜ L·ª¢I √çCH",
        "Chuong_8": "Ch∆∞∆°ng 8: C√îNG NGHI·ªÜP H√ìA, HI·ªÜN ƒê·∫†I H√ìA ·ªû VI·ªÜT NAM",
        "Chuong_9": "Ch∆∞∆°ng 9: H·ªòI NH·∫¨P KINH T·∫æ QU·ªêC T·∫æ & X√ÇY D·ª∞NG N·ªÄN KINH T·∫æ ƒê·ªòC L·∫¨P T·ª∞ CH·ª¶"
    }
    return mapping.get(name, name)

# Xu ly cau hoi (Streaming) - KHONG session_id
async def response_generator(query: str):
    # Kiem tra neu Engine chua san sang
    if GLOBAL_ENGINE is None:
        yield "H·ªá th·ªëng ƒëang kh·ªüi ƒë·ªông ho·∫∑c ch∆∞a c√≥ d·ªØ li·ªáu."
        return

    try:
        # Kiem tra cache
        cache_key = normalize_query_key(query)
        if cache_key in response_cache:
            yield response_cache[cache_key]
            return

        # Goi AI tu Global Engine
        try:
            streaming_response = await GLOBAL_ENGINE.astream_chat(query)
            full_text = ""
            
            async for token in streaming_response.async_response_gen():
                full_text += token
                yield token
            
            # Xu ly nguon
            final_resp = str(streaming_response)
            refusals = ["kh√¥ng c√≥ th√¥ng tin", "kh√¥ng ƒë∆∞·ª£c ƒë·ªÅ c·∫≠p", "t√¥i kh√¥ng bi·∫øt", "xin l·ªói"]
            is_refusal = any(p in final_resp.lower() for p in refusals)
            
            clean_sources = []
            seen = set()
            if not is_refusal and streaming_response.source_nodes:
                for node in streaming_response.source_nodes:
                    fname = node.node.metadata.get('file_name', '')
                    if not fname or any(x in fname for x in ["Loi_Mo_Dau", "C00", "C0_"]): continue
                    if fname not in seen:
                        seen.add(fname)
                        clean_sources.append(prettify_source(fname))
            
            if clean_sources:
                src_html = "<br>".join([f"üìñ <i>{src}</i>" for src in clean_sources[:3]])
                append_text = f"\n\n<hr><b>üìö Ngu·ªìn tham kh·∫£o:</b><br>{src_html}"
                yield append_text
                full_text += append_text
            
            # Luu cache
            response_cache[cache_key] = full_text
            save_cache()

        except ServerError:
            yield "L·ªói: Server b·∫≠n (503)."
        except Exception as e:
            yield f"L·ªói: {str(e)}"

    except Exception as e:
        yield f"L·ªói h·ªá th·ªëng: {str(e)}"

# API Endpoint - KHONG session_id
class QueryRequest(BaseModel):
    query: str

@app.post("/api/query")
async def handle_query(request: QueryRequest):
    return StreamingResponse(response_generator(request.query), media_type="text/plain")

if os.path.exists("static"):
    app.mount("/", StaticFiles(directory="static", html=True), name="frontend")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)